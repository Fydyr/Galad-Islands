#!/usr/bin/env python3
"""
Script d'entra√Ænement unifi√© pour l'IA de la base Galad Islands.
Combine sc√©narios strat√©giques, self-play et objectif de victoire.
"""
import sys
import os
import time
import random
import numpy as np
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
import joblib
# Ajouter le r√©pertoire src au path
sys.path.insert(0, str(Path(__file__).parent / "src"))
from ia.BaseAi import BaseAi
from constants.gameplay import UNIT_COSTS

MODEL_DIR = "src/models"
MODEL_PATH = f"{MODEL_DIR}/base_ai_unified_final.pkl"

class UnifiedBaseAiTrainer:
    def __init__(self, team_id=2):
        self.gold_reserve = 50
        self.max_turns = 120
        self.team_id = team_id

    def generate_scenario_examples(self, n_per_scenario=200):
        """G√©n√®re des exemples pour chaque sc√©nario strat√©gique cl√©, y compris la p√©nurie d'or. Exploration et D√©fense prioritaire sont surrepr√©sent√©s et surpond√©r√©s."""
        scenarios = [
            # (gold, base_health, allied_units, enemy_units, enemy_base_known, towers_needed, enemy_base_health, expected_action, repeat, reward_val)            
            # --- NOUVELLE STRAT√âGIE : PHASE D'EXPLORATION ---
            # Si la base ennemie est inconnue, la priorit√© absolue est de cr√©er des √©claireurs.
            (100, 1.0, 1, 1, 0, 0, 1.0, 1, 20, 600),  # Exploration n√©cessaire (√âclaireur), poids augment√© (x20, r√©compense 600)
            
            # --- NOUVELLE STRAT√âGIE : PHASE D'ASSAUT (POST-D√âCOUVERTE) ---
            # Une fois la base ennemie connue et avec un avantage √©conomique, on investit dans des unit√©s lourdes.
            (350, 0.9, 10, 2, 1, 0, 1.0, 4, 10, 500),  # Avantage √©conomique -> L√©viathan (action 4), poids augment√© (x10)
            
            # --- SC√âNARIOS EXISTANTS ---
            # D√©fense prioritaire raffin√©e : base tr√®s basse ET forte inf√©riorit√© num√©rique
            (150, 0.35, 2, 7, 1, 1, 1.0, 6, 20, 700),  # D√©fense extr√™me : attaque Kamikaze x20
            (120, 0.3, 1, 8, 1, 1, 1.0, 6, 10, 700),   # Variante extr√™me : attaque Kamikaze x10
            (150, 0.5, 3, 6, 1, 1, 1.0, 3, 5, 400),    # D√©fense mod√©r√©e : Maraudeur x5
            (150, 0.7, 4, 7, 1, 1, 1.0, 3, 1, 120),   # Inf√©riorit√© num√©rique (Maraudeur)
            (120, 0.8, 2, 4, 1, 0, 1.0, 6, 1, 120),   # Contre-attaque rapide (Kamikaze)
            (150, 0.9, 3, 2, 1, 0, 0.1, 6, 1, 120),   # Coup de gr√¢ce (Kamikaze)
            (10, 0.8, 2, 2, 1, 0, 1.0, 0, 1, 120),    # P√©nurie d'or, IA doit √©conomiser (Rien)
        ]
        states, actions, rewards = [], [], []
        for gold, base_health, allied_units, enemy_units, enemy_base_known, towers_needed, enemy_base_health, expected_action, repeat, reward_val in scenarios:
            for _ in range(n_per_scenario):
                g = gold + np.random.randint(-20, 21)
                bh = np.clip(base_health + np.random.normal(0, 0.05), 0.1, 1.0)
                au = max(0, allied_units + np.random.randint(-1, 2))
                eu = max(0, enemy_units + np.random.randint(-1, 2))
                ebk = enemy_base_known if np.random.rand() > 0.1 else 1 - enemy_base_known
                tn = towers_needed if np.random.rand() > 0.1 else 1 - towers_needed
                ebh = np.clip(enemy_base_health + np.random.normal(0, 0.05), 0.05, 1.0)
                allied_health = bh if au > 0 else 1.0
                state_action = [g, bh, au, eu, ebk, tn, ebh, allied_health, expected_action]
                for _ in range(repeat):
                    states.append(state_action)
                    actions.append(expected_action)
                    rewards.append(reward_val)
                for wrong_action in range(7):
                    if wrong_action != expected_action:
                        wrong_state_action = [g, bh, au, eu, ebk, tn, ebh, allied_health, wrong_action]
                        for _ in range(repeat):
                            states.append(wrong_state_action)
                            actions.append(wrong_action)
                            rewards.append(-100 if expected_action in (1,2) else -30)
        return states, actions, rewards

    def simulate_self_play_game(self, ai1, ai2, bonus_victory=200):
        """Simule une partie entre deux IA, avec bonus fort √† la victoire."""
        game_state = {
            'ally_gold': 50,  # Mise √† jour pour correspondre √† PLAYER_DEFAULT_GOLD
            'enemy_gold': 50, # Mise √† jour pour correspondre √† PLAYER_DEFAULT_GOLD
            'ally_base_health': 1.0,
            'enemy_base_health': 1.0,
            'ally_units': 1,
            'enemy_units': 1,
            'ally_architects': 0,  # NOUVEAU: Suivi des architectes
            'enemy_architects': 0, # NOUVEAU: Suivi des architectes
            'ally_towers_needed': 0,
            'enemy_towers_needed': 0,
            'enemy_base_known_ally': 0,
            'enemy_base_known_enemy': 0,
            'turn': 0
        }
        ai1_experiences, ai2_experiences = [], []
        while game_state['turn'] < self.max_turns:
            game_state['turn'] += 1
            # IA 1 (alli√©s)
            if game_state['ally_base_health'] > 0:
                state_ally = self._get_state_for_ai(game_state, is_ally=True)
                action_ally = self.decide_action_for_training(*state_ally)
                reward_ally = self._apply_action_simulation(action_ally, game_state, is_ally=True)
                state_action_ally = state_ally + [action_ally]
                ai1_experiences.append((state_action_ally, reward_ally))
            # IA 2 (ennemis)
            if game_state['enemy_base_health'] > 0:
                state_enemy = self._get_state_for_ai(game_state, is_ally=False)
                action_enemy = self.decide_action_for_training(*state_enemy)
                reward_enemy = self._apply_action_simulation(action_enemy, game_state, is_ally=False)
                state_action_enemy = state_enemy + [action_enemy]
                ai2_experiences.append((state_action_enemy, reward_enemy))
            self._evolve_world(game_state)
            if game_state['ally_base_health'] <= 0 or game_state['enemy_base_health'] <= 0:
                if game_state['ally_base_health'] <= 0:
                    for i, (sa, r) in enumerate(ai2_experiences):
                        ai2_experiences[i] = (sa, r + bonus_victory)
                    for i, (sa, r) in enumerate(ai1_experiences):
                        ai1_experiences[i] = (sa, r - bonus_victory)
                elif game_state['enemy_base_health'] <= 0:
                    for i, (sa, r) in enumerate(ai1_experiences):
                        ai1_experiences[i] = (sa, r + bonus_victory)
                    for i, (sa, r) in enumerate(ai2_experiences):
                        ai2_experiences[i] = (sa, r - bonus_victory)
                break
        return ai1_experiences, ai2_experiences

    def decide_action_for_training(self, gold, base_health_ratio, allied_units, enemy_units, enemy_base_known, towers_needed, enemy_base_health_ratio, allied_units_health):
        """Logique de d√©cision √† base de r√®gles pour l'entra√Ænement."""
        # Priorit√© absolue : d√©fense si la base est tr√®s faible et en inf√©riorit√©
        if base_health_ratio < 0.4 and allied_units < enemy_units + 2:
            if gold >= UNIT_COSTS["kamikaze"] + self.gold_reserve:
                return 6  # Kamikaze pour une d√©fense rapide
            if gold >= UNIT_COSTS["maraudeur"] + self.gold_reserve:
                return 3  # Maraudeur

        # Exploration si la base ennemie est inconnue
        if not enemy_base_known:
            if gold >= UNIT_COSTS["scout"]:
                return 1  # √âclaireur

        # Soin si les unit√©s sont bless√©es
        if allied_units_health < 0.5 and allied_units > 3:
            if gold >= UNIT_COSTS["druid"] + self.gold_reserve:
                return 5 # Druide

        # Coup de gr√¢ce si la base ennemie est faible
        if enemy_base_health_ratio < 0.2:
            if gold >= UNIT_COSTS["kamikaze"] + self.gold_reserve:
                return 6  # Kamikaze

        # Strat√©gie g√©n√©rale
        if allied_units < enemy_units:
            # Renforcer avec des unit√©s de combat
            if gold >= UNIT_COSTS["maraudeur"] + self.gold_reserve:
                return 3  # Maraudeur
            if gold >= UNIT_COSTS["kamikaze"] + self.gold_reserve:
                return 6  # Kamikaze
        elif gold > 300 and allied_units > enemy_units + 3:
            # Avantage √©conomique et militaire -> unit√© lourde
            if gold >= UNIT_COSTS["leviathan"] + self.gold_reserve:
                return 4  # L√©viathan
        
        # Si aucune condition n'est remplie, √©conomiser
        if gold < 100:
            return 0 # Ne rien faire

        return random.choices([0, 1, 3, 6], weights=[0.4, 0.1, 0.3, 0.2], k=1)[0]

    def _get_state_for_ai(self, game_state, is_ally=True):
        """Retourne un vecteur d'√©tat pour `decide_action_for_training`.
        Ajoute en fin la valeur `allied_units_health` pour compatibilit√© avec la nouvelle signature.
        """
        allied_units_health_default = 1.0
        # NOTE: Le nombre d'architectes n'est pas encore une feature du mod√®le, donc on ne l'ajoute pas ici.
        if is_ally:
            return [game_state['ally_gold'], game_state['ally_base_health'], game_state['ally_units'], game_state['enemy_units'], game_state['enemy_base_known_ally'], game_state['ally_towers_needed'], game_state['enemy_base_health'], allied_units_health_default]
        else:
            return [game_state['enemy_gold'], game_state['enemy_base_health'], game_state['enemy_units'], game_state['ally_units'], game_state['enemy_base_known_enemy'], game_state['enemy_towers_needed'], game_state['ally_base_health'], allied_units_health_default]

    def _apply_action_simulation(self, action, game_state, is_ally=True):
        if is_ally:
            gold_key, units_key, architects_key, towers_key, enemy_health_key, enemy_known_key = 'ally_gold', 'ally_units', 'ally_architects', 'ally_towers_needed', 'enemy_base_health', 'enemy_base_known_ally'
        else:
            gold_key, units_key, architects_key, towers_key, enemy_health_key, enemy_known_key = 'enemy_gold', 'enemy_units', 'enemy_architects', 'enemy_towers_needed', 'ally_base_health', 'enemy_base_known_enemy'
        gold = game_state[gold_key]; reward = 0
        if action == 0:
            if gold > 500: reward += 15
            elif gold > 250: reward += 5
        cost = 0
        if action == 1: cost = UNIT_COSTS["scout"]
        elif action == 2: cost = UNIT_COSTS["architect"]
        elif action == 3: cost = UNIT_COSTS["maraudeur"]
        elif action == 4: cost = UNIT_COSTS["leviathan"]
        elif action == 5: cost = UNIT_COSTS["druid"]
        elif action == 6: cost = UNIT_COSTS["kamikaze"]
        if action != 0 and gold >= cost:
            game_state[gold_key] -= cost
            reward += cost / 15 # R√©compense proportionnelle au co√ªt

            # G√©rer la cr√©ation d'architecte s√©par√©ment
            if action == 2: # Action pour cr√©er un Architecte
                game_state[architects_key] += 1
            else:
                game_state[units_key] += 1

            if action == 1:
                game_state[enemy_known_key] = 1; reward += 5
            elif action == 6:
                game_state[enemy_health_key] -= 0.15; reward += 15
        return reward

    def _evolve_world(self, game_state):
        # Collecte active : chaque unit√© alli√©e rapporte 8 √† 15 or par tour
        # --- Simulation de l'Architecte ---
        # 1. Collecte d'or
        for _ in range(game_state['ally_architects']):
            if random.random() < 0.2: # 20% de chance par tour de trouver un coffre
                game_state['ally_gold'] += random.randint(60, 150) # Gain d'un coffre volant
        for _ in range(game_state['enemy_architects']):
            if random.random() < 0.2:
                game_state['enemy_gold'] += random.randint(60, 150)
        
        # 2. D√©pense d'or pour les tours (NOUVEAU)
        for _ in range(game_state['ally_architects']):
            if random.random() < 0.1 and game_state['ally_gold'] > 150: # 10% de chance de construire une tour si l'or est suffisant
                game_state['ally_gold'] -= UNIT_COSTS["attack_tower"]
        for _ in range(game_state['enemy_architects']):
            if random.random() < 0.1 and game_state['enemy_gold'] > 150:
                game_state['enemy_gold'] -= UNIT_COSTS["attack_tower"]

        game_state['ally_gold'] += sum([random.randint(8, 15) for _ in range(game_state['ally_units'])])
        game_state['enemy_gold'] += sum([random.randint(8, 15) for _ in range(game_state['enemy_units'])])
        # Si aucune unit√©, tr√®s faible revenu (tr√©sor de base)
        if game_state['ally_units'] == 0:
            game_state['ally_gold'] += random.randint(1, 3)
        if game_state['enemy_units'] == 0:
            game_state['enemy_gold'] += random.randint(1, 3)
        advantage = game_state['ally_units'] - game_state['enemy_units']
        loss_chance_ally = 0.25 - 0.05 * advantage
        if random.random() < loss_chance_ally:
            losses = random.randint(1, max(1, game_state['ally_units'] // 3))
            game_state['ally_units'] = max(0, game_state['ally_units'] - losses)
            game_state['ally_base_health'] -= losses * 0.02
        loss_chance_enemy = 0.25 + 0.05 * advantage
        if random.random() < loss_chance_enemy:
            losses = random.randint(1, max(1, game_state['enemy_units'] // 3))
            game_state['enemy_units'] = max(0, game_state['enemy_units'] - losses)
            if game_state['enemy_base_known_ally']:
                game_state['enemy_base_health'] -= losses * 0.02
        if not game_state['enemy_base_known_ally'] and game_state['ally_units'] > 5 and random.random() < 0.2:
            game_state['enemy_base_known_ally'] = 1
            game_state['enemy_base_known_enemy'] = 1
        game_state['ally_base_health'] = max(0, min(1.0, game_state['ally_base_health']))
        game_state['enemy_base_health'] = max(0, min(1.0, game_state['enemy_base_health']))

    def generate_victory_scenario(self, n_games=100):
        """G√©n√®re des parties o√π l'IA doit gagner (objectif explicite)."""
        ai1 = BaseAi(team_id=1)
        ai2 = BaseAi(team_id=2)
        all_experiences = []
        for _ in range(n_games):
            exp1, exp2 = self.simulate_self_play_game(ai1, ai2, bonus_victory=500)
            all_experiences.extend(exp1)
        states_actions = [exp[0] for exp in all_experiences]
        rewards = [exp[1] for exp in all_experiences]
        return states_actions, rewards

    def train_unified_model(self, n_scenarios=200, n_selfplay=100, n_victory=100, n_iterations=3):
        print("\nüöÄ ENTRA√éNEMENT UNIFI√â DE L'IA DE BASE")
        print("=" * 60)
        all_states, all_rewards = [], []
        autosave_path = f"{MODEL_DIR}/base_ai_unified_final_autosave.npz"
        # Reprise si autosave existant
        if os.path.exists(autosave_path):
            print(f"‚ö†Ô∏è Fichier autosave {autosave_path} d√©tect√©. Reprise possible.")
            try:
                data = np.load(autosave_path, allow_pickle=True)
                all_states = list(data['states'])
                all_rewards = list(data['rewards'])
                print(f"‚û°Ô∏è Reprise avec {len(all_states)} exemples d√©j√† collect√©s.")
            except Exception as e:
                print(f"Erreur lors du chargement du fichier autosave : {e}")

        try:
            # 1. Exemples sc√©narios strat√©giques
            s_states, s_actions, s_rewards = self.generate_scenario_examples(n_per_scenario=n_scenarios)
            all_states.extend(s_states)
            all_rewards.extend(s_rewards)
            print(f"  Progression: Sc√©narios strat√©giques ({len(all_states)} exemples)")
            # Sauvegarde auto apr√®s sc√©narios
            np.savez_compressed(autosave_path, states=np.array(all_states, dtype=object), rewards=np.array(all_rewards, dtype=object))
            # 2. Self-play classique
            ai1 = BaseAi(team_id=1)
            ai2 = BaseAi(team_id=2)
            total_selfplay = n_selfplay * n_iterations
            last_percent = -1
            for it in range(n_iterations):
                print(f"\nüîÑ Self-play it√©ration {it+1}/{n_iterations}")
                for sp in range(n_selfplay):
                    exp1, exp2 = self.simulate_self_play_game(ai1, ai2)
                    all_states.extend([e[0] for e in exp1])
                    all_rewards.extend([e[1] for e in exp1])
                    all_states.extend([e[0] for e in exp2])
                    all_rewards.extend([e[1] for e in exp2])
                    percent = int(100 * ((it * n_selfplay + sp + 1) / total_selfplay))
                    if percent != last_percent:
                        print(f"  Progression self-play: {percent}% ({it * n_selfplay + sp + 1}/{total_selfplay})")
                        last_percent = percent
                    # Sauvegarde auto √† chaque palier de 5%
                    if percent % 5 == 0 and (it * n_selfplay + sp + 1) % max(1, total_selfplay // 20) == 0:
                        np.savez_compressed(autosave_path, states=np.array(all_states, dtype=object), rewards=np.array(all_rewards, dtype=object))
                        print(f"üíæ Sauvegarde auto ({percent}%) : {len(all_states)} exemples.")
            # 3. Sc√©nario victoire obligatoire
            v_states, v_rewards = self.generate_victory_scenario(n_games=n_victory)
            all_states.extend(v_states)
            all_rewards.extend(v_rewards)
            print(f"  Progression: Sc√©narios victoire ({len(all_states)} exemples)")
            np.savez_compressed(autosave_path, states=np.array(all_states, dtype=object), rewards=np.array(all_rewards, dtype=object))
        except KeyboardInterrupt:
            print("\n‚èπÔ∏è Interruption utilisateur (Ctrl+C) : sauvegarde des donn√©es d'entra√Ænement...")
            np.savez_compressed(autosave_path, states=np.array(all_states, dtype=object), rewards=np.array(all_rewards, dtype=object))
            print(f"‚úÖ Donn√©es sauvegard√©es dans {autosave_path} apr√®s interruption.")
            raise
        except Exception as e:
            print(f"\nüí• Exception inattendue : {e}\nSauvegarde des donn√©es d'entra√Ænement...")
            np.savez_compressed(autosave_path, states=np.array(all_states, dtype=object), rewards=np.array(all_rewards, dtype=object))
            print(f"‚úÖ Donn√©es sauvegard√©es dans {autosave_path} apr√®s crash.")
            raise
        finally:
            if all_states:
                print("\nüíæ Sauvegarde automatique des donn√©es d'entra√Ænement (sortie/crash/interruption)...")
                np.savez_compressed(autosave_path, states=np.array(all_states, dtype=object), rewards=np.array(all_rewards, dtype=object))
                print(f"‚úÖ Donn√©es sauvegard√©es dans {autosave_path} (finally).")

        print(f"\nüìà Total exemples: {len(all_states)}")
        # Entra√Ænement du mod√®le
        X = np.array(all_states)
        y = np.array(all_rewards)

        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        model = RandomForestRegressor(n_estimators=40, max_depth=10, min_samples_split=20, min_samples_leaf=10, random_state=42, n_jobs=-1)
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        mse = mean_squared_error(y_test, y_pred)
        print(f"‚úÖ Entra√Ænement termin√©. MSE: {mse:.2f}")
        os.makedirs(MODEL_DIR, exist_ok=True)
        joblib.dump(model, MODEL_PATH)
        print(f"üíæ Mod√®le sauvegard√©: {MODEL_PATH}")
        print("=" * 60)
        print("‚ú® ENTRA√éNEMENT UNIFI√â TERMIN√â !")
        return model, mse

def main():
    print("ü§ñ Entra√Æneur IA unifi√© pour Galad Islands\n")
    import argparse
    parser = argparse.ArgumentParser(description='Entra√Æneur unifi√© BaseAi pour Galad Islands')
    parser.add_argument('--n_scenarios', type=int, default=2000, help='Nombre de sc√©narios strat√©giques par type (d√©faut: 2000)')
    parser.add_argument('--n_selfplay', type=int, default=1000, help='Nombre de parties de self-play (d√©faut: 1000)')
    parser.add_argument('--n_victory', type=int, default=500, help='Nombre de parties victoire pour renforcer l objectif (d√©faut: 500)')
    parser.add_argument('--n_iterations', type=int, default=5, help='It√©rations de self-play (d√©faut: 5)')
    parser.add_argument('--seed', type=int, default=42, help='Graine al√©atoire pour reproductibilit√© (d√©faut: 42)')
    args = parser.parse_args()
    random.seed(args.seed)
    np.random.seed(args.seed)

    trainer = UnifiedBaseAiTrainer()
    model, mse = trainer.train_unified_model(
        n_scenarios=args.n_scenarios, n_selfplay=args.n_selfplay, n_victory=args.n_victory, n_iterations=args.n_iterations
    )
    print("\nüéÆ Le mod√®le unifi√© est pr√™t √† √™tre utilis√© dans le jeu!")
    print("üí° Pour l'utiliser, modifiez BaseAi.load_or_train_model() pour charger base_ai_unified_final.pkl.")

if __name__ == "__main__":
    main()
