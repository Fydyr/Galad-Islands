#!/usr/bin/env python3
"""
Script d'entra√Ænement par self-play pour l'IA de la base.
Les IA s'affrontent mutuellement pour s'am√©liorer via reinforcement learning.
"""

import sys
import os
import time
import random
import numpy as np
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.tree import DecisionTreeRegressor
import joblib

# Ajouter le r√©pertoire src au path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from ia.BaseAi import BaseAi
from constants.gameplay import UNIT_COSTS


class SelfPlayBaseAiTrainer:
    """Entra√Æneur utilisant le self-play pour am√©liorer l'IA de la base."""

    def __init__(self):
        self.gold_reserve = 50
        self.max_turns = 100  # Limite de tours par partie

    def simulate_self_play_game(self, ai1, ai2):
        """
        Simule une partie entre deux IA.
        Retourne les exp√©riences collect√©es pour chaque IA.
        """
        # √âtat initial
        game_state = {
            'ally_gold': 100,
            'enemy_gold': 100,
            'ally_base_health': 1.0,
            'enemy_base_health': 1.0,
            'ally_units': 1,
            'enemy_units': 1,
            'ally_towers_needed': 0,
            'enemy_towers_needed': 0,
            'enemy_base_known_ally': 0,
            'enemy_base_known_enemy': 0,
            'turn': 0
        }

        ai1_experiences = []  # (state_action, reward)
        ai2_experiences = []

        while game_state['turn'] < self.max_turns:
            game_state['turn'] += 1

            # Tour de l'IA 1 (alli√©s)
            if game_state['ally_base_health'] > 0:
                state_ally = self._get_state_for_ai(game_state, is_ally=True)
                action_ally = ai1.decide_action_for_training(*state_ally)
                reward_ally = self._apply_action_simulation(action_ally, game_state, is_ally=True)

                # Sauvegarder l'exp√©rience pour l'IA 1
                state_action_ally = state_ally + [action_ally]
                ai1_experiences.append((state_action_ally, reward_ally))

            # Tour de l'IA 2 (ennemis)
            if game_state['enemy_base_health'] > 0:
                state_enemy = self._get_state_for_ai(game_state, is_ally=False)
                action_enemy = ai2.decide_action_for_training(*state_enemy)
                reward_enemy = self._apply_action_simulation(action_enemy, game_state, is_ally=False)

                # Sauvegarder l'exp√©rience pour l'IA 2
                state_action_enemy = state_enemy + [action_enemy]
                ai2_experiences.append((state_action_enemy, reward_enemy))

            # √âvolution du monde entre les tours
            self._evolve_world(game_state)

            # Conditions de fin
            if game_state['ally_base_health'] <= 0 or game_state['enemy_base_health'] <= 0:
                # R√©compenses finales
                if game_state['ally_base_health'] <= 0:
                    # Victoire de l'IA 2
                    for i, (sa, r) in enumerate(ai2_experiences):
                        ai2_experiences[i] = (sa, r + 50)  # Bonus de victoire
                    for i, (sa, r) in enumerate(ai1_experiences):
                        ai1_experiences[i] = (sa, r - 50)  # P√©nalit√© de d√©faite
                elif game_state['enemy_base_health'] <= 0:
                    # Victoire de l'IA 1
                    for i, (sa, r) in enumerate(ai1_experiences):
                        ai1_experiences[i] = (sa, r + 50)  # Bonus de victoire
                    for i, (sa, r) in enumerate(ai2_experiences):
                        ai2_experiences[i] = (sa, r - 50)  # P√©nalit√© de d√©faite
                break

        return ai1_experiences, ai2_experiences

    def _get_state_for_ai(self, game_state, is_ally=True):
        """Extrait l'√©tat du jeu pour une IA sp√©cifique."""
        if is_ally:
            return [
                game_state['ally_gold'],
                game_state['ally_base_health'],
                game_state['ally_units'],
                game_state['enemy_units'],
                game_state['enemy_base_known_ally'],
                game_state['ally_towers_needed'],
                game_state['enemy_base_health']
            ]
        else:
            return [
                game_state['enemy_gold'],
                game_state['enemy_base_health'],
                game_state['enemy_units'],
                game_state['ally_units'],
                game_state['enemy_base_known_enemy'],
                game_state['enemy_towers_needed'],
                game_state['ally_base_health']
            ]

    def _apply_action_simulation(self, action, game_state, is_ally=True):
        """Applique une action et retourne la r√©compense."""
        if is_ally:
            gold_key = 'ally_gold'
            units_key = 'ally_units'
            towers_key = 'ally_towers_needed'
            enemy_health_key = 'enemy_base_health'
            enemy_known_key = 'enemy_base_known_ally'
        else:
            gold_key = 'enemy_gold'
            units_key = 'enemy_units'
            towers_key = 'enemy_towers_needed'
            enemy_health_key = 'ally_base_health'
            enemy_known_key = 'enemy_base_known_enemy'

        gold = game_state[gold_key]
        reward = -1  # Co√ªt l√©ger par action

        if action == 0:
            # P√©nalit√© pour "Rien" si on a de l'or
            if gold >= 30:
                reward -= 5
        elif action == 1 and gold >= 30:  # √âclaireur
            game_state[gold_key] -= 30
            game_state[units_key] += 1
            game_state[enemy_known_key] = 1
            reward += 10  # R√©compense pour exploration
        elif action == 2 and gold >= 50:  # Architecte
            game_state[gold_key] -= 50
            game_state[towers_key] = max(0, game_state[towers_key] - 1)
            reward += 15  # R√©compense pour d√©fense
        elif action == 3 and gold >= 40:  # Maraudeur
            game_state[gold_key] -= 40
            game_state[units_key] += 1
            reward += 12  # R√©compense pour unit√© d'attaque
        elif action == 4 and gold >= 120:  # L√©viathan
            game_state[gold_key] -= 120
            game_state[units_key] += 1
            reward += 20  # Grande r√©compense pour unit√© lourde
        elif action == 5 and gold >= 80:  # Druide
            game_state[gold_key] -= 80
            game_state[units_key] += 1
            reward += 18  # R√©compense pour unit√© de soin
        elif action == 6 and gold >= 60:  # Kamikaze
            game_state[gold_key] -= 60
            game_state[enemy_health_key] -= 0.2
            reward += 25  # Grande r√©compense pour attaque directe

        return reward

    def _evolve_world(self, game_state):
        """Fait √©voluer le monde entre les tours."""
        # Revenus
        game_state['ally_gold'] += random.randint(15, 30)
        game_state['enemy_gold'] += random.randint(15, 30)

        # Production ennemie (simplifi√©e)
        if random.random() < 0.3:
            game_state['enemy_units'] += 1
        if random.random() < 0.3:
            game_state['ally_units'] += 1

        # Combats al√©atoires
        if random.random() < 0.4:
            damage = random.uniform(0.05, 0.15)
            if random.random() < 0.5:
                game_state['ally_base_health'] -= damage
            else:
                game_state['enemy_base_health'] -= damage

        # Limites
        game_state['ally_base_health'] = max(0, min(1.0, game_state['ally_base_health']))
        game_state['enemy_base_health'] = max(0, min(1.0, game_state['enemy_base_health']))

    def train_self_play(self, n_games=100, n_iterations=5):
        """Entra√Æne les IA par self-play sur plusieurs it√©rations."""
        print("üéÆ D√âBUT DE L'ENTRA√éNEMENT PAR SELF-PLAY")
        print("=" * 60)

        # Initialiser deux IA
        ai1 = BaseAi(team_id=1)
        ai2 = BaseAi(team_id=2)

        for iteration in range(n_iterations):
            print(f"\nüîÑ It√©ration {iteration + 1}/{n_iterations}")
            start_time = time.time()

            all_experiences = []

            # Jouer plusieurs parties
            for game in range(n_games):
                exp1, exp2 = self.simulate_self_play_game(ai1, ai2)
                all_experiences.extend(exp1)
                all_experiences.extend(exp2)

                if (game + 1) % 20 == 0:
                    print(f"  ‚úÖ Parties jou√©es: {game + 1}/{n_games}")

            # Pr√©parer les donn√©es d'entra√Ænement
            states_actions = [exp[0] for exp in all_experiences]
            rewards = [exp[1] for exp in all_experiences]

            X = np.array(states_actions)
            y = np.array(rewards)

            # Entra√Æner un nouveau mod√®le
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

            new_model = DecisionTreeRegressor(max_depth=8, random_state=42)
            new_model.fit(X_train, y_train)

            y_pred = new_model.predict(X_test)
            mse = mean_squared_error(y_test, y_pred)

            training_time = time.time() - start_time
            print(f"‚è∞ Temps d'it√©ration: {training_time:.2f} secondes")
            print(f"üéØ MSE: {mse:.3f}")
            print(f"üìä Donn√©es collect√©es: {len(states_actions)} exp√©riences")

            # Mettre √† jour les mod√®les des IA
            ai1.model = new_model
            ai2.model = new_model

            # Sauvegarder le mod√®le
            model_path = f"src/models/base_ai_selfplay_iter_{iteration + 1}.pkl"
            os.makedirs("src/models", exist_ok=True)
            joblib.dump(new_model, model_path)
            print(f"üíæ Mod√®le sauvegard√©: {model_path}")

        # Sauvegarder le mod√®le final
        final_model_path = "src/models/base_ai_selfplay_final.pkl"
        joblib.dump(ai1.model, final_model_path)
        print(f"\nüèÜ Mod√®le final sauvegard√©: {final_model_path}")
        print("=" * 60)
        print("‚ú® ENTRA√éNEMENT PAR SELF-PLAY TERMIN√â !")
        print("=" * 60)


def main():
    """Fonction principale."""
    print("ü§ñ Entra√Æneur Self-Play pour l'IA de Base - Galad Islands")
    print()

    # Demander les param√®tres
    try:
        n_games = int(input("Nombre de parties par it√©ration (d√©faut: 50): ") or "50")
        n_iterations = int(input("Nombre d'it√©rations (d√©faut: 3): ") or "3")
    except ValueError:
        n_games = 50
        n_iterations = 3

    print(f"üî• Lancement du self-play: {n_games} parties x {n_iterations} it√©rations...")
    print()

    trainer = SelfPlayBaseAiTrainer()
    trainer.train_self_play(n_games, n_iterations)

    print()
    print("üéÆ Les IA se sont affront√©es et am√©lior√©es mutuellement!")
    print("üí° Utilisez le mod√®le final dans BaseAi.load_or_train_model() pour charger le mod√®le avanc√©.")


if __name__ == "__main__":
    main()